---
permalink: /
title: "About me"
excerpt: "About me"
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---


I am a second-year PhD student in the [Center for Information and Language Processing](https://www.cis.uni-muenchen.de/) at [LMU Munich](https://www.en.uni-muenchen.de/index.html) under the supervision of [Alex Fraser](https://www.cis.uni-muenchen.de/~fraser/). I am broadly interested in transfer learning, self-supervised training for low-resource machine translation, in both supervised and unsupervised scenarios, as well as domain adaptation. My PhD thesis is currently focused on improving cross-lingual pretraining for low-resource languages. In this line of work, I proposed a curriculum for pretraining masked language models to improve unsupervised translation for high-resource/low-resource language pairs. I have also worked on enhancing cross-lingual pretraining by including lexical information from non-contextualized embeddings. Lately, I have been working on multilingual neural machine translation for low-resource directions. 

 Before that, I completed my diploma (combined BEng and MEng) at the [National Technical University of Athens](https://www.ntua.gr/en/) in Athens, Greece, department of [Electrical and Computer Engineering](https://www.ece.ntua.gr/en). In my thesis, I explored ways of improving transfer learning with language Modeling for several classification tasks, most notably on emotion recognition, under the supervision of [Alex Potamianos](https://scholar.google.com/citations?user=pBQViyUAAAAJ&hl=en). 

 <h2>News</h2>

<b>March 2021</b>: Our paper "Improving the Lexical Ability of Pretrained Language Models for Unsupervised Neural Machine Translation" has been accepted to appear in NAACL 2021!

<b>September 2020</b>: Our paper "Reusing a Pretrained Language Model on Languages with Limited Corpora for Unsupervised NMT" has been accepted to appear in EMNLP 2020! Also, our work "Domain Adversarial Fine-Tuning as an Effective Regularizer" was accepted in the findings of EMNLP.

<b>July 2020</b>: Our system ranked first in the WMT 2020 Unsupervised Translation Shared Task. The task was to create a purely unsupervised machine translation model that translates between Upper Sorbian and German.

