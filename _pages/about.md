---
permalink: /
title: "About"
excerpt: "Alexandra Chronopoulou"
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---


I am Alexandra, a Research Scientist at Google in the [Bard](https://bard.google.com/chat) team working on Natural Language Processing (Machine Learning). 

Before that, I was a PhD student at the [University of Munich](https://www.en.uni-muenchen.de/index.html) supervised by [Alex Fraser](https://www.cis.uni-muenchen.de/~fraser/). My PhD research was mostly on combining information from various languages and domains to enable positive transfer during parameter-efficient fine-tuning of language models, especially under resource constraints.   

<!-- machine translation, multilinguality and domain adaptation. I am currently interested in parameter-efficient fine-tuning methods; I am particularly excited about combining information from different languages, domains or tasks to enable positive transfer using modular approaches. -->

During my PhD, I interned at [Google DeepÎœind](https://www.deepmind.com/) in Berlin, hosted by [Sebastian Ruder](https://www.ruder.io/) and [Priyanka Agrawal](https://sites.google.com/site/priyankaagr17). Prior to that, I interned (twice) at the [Allen Institute for AI](https://allenai.org/) with [Jesse Dodge](https://jessedodge.github.io/) and [Matt Peters](https://scholar.google.com/citations?user=K5nCPZwAAAAJ&hl=en); I was part of the [AllenNLP](https://allenai.org/allennlp) team. I also spent 4 months at [Amazon AI](https://aws.amazon.com/machine-learning/) in Santa Clara, CA , working with [Brian Thompson](https://thompsonb.github.io/), [Prashant Mathur](http://mtresearcher.github.io/) and [Marcello Federico](https://www.marcellofederico.net) as an intern in the AI human language technology group.

Before starting the PhD, I graduated with a diploma (Bachelor and MEng) in [Electrical & Computer Engineering](https://www.ece.ntua.gr/en) from the [National Technical University of Athens (NTUA)](https://www.ntua.gr/en/). 



 <h2>News</h2>
 
  <b>January 2024</b>: Excited to share that I have joined Google Bard in NYC as a Research Scientist!

 <b>November 2023</b>: New preprint is out on [Language and Task Arithmetic with Parameter-Efficient Layers for Zero-Shot Summarization
](https://arxiv.org/abs/2311.09344.pdf) from my internship at Google DeepMind!


 <b>October 2023</b>: Happy to share that our paper [Mitigating Data Imbalance and Representation Degeneration in Multilingual Machine Translation](https://arxiv.org/pdf/2305.12786.pdf) has been accepted to the Findings of EMNLP 2023!

 <b>May 2023</b>: Our paper [Mitigating Data Imbalance and Representation Degeneration in Multilingual Machine Translation](https://arxiv.org/pdf/2305.12786.pdf) is out.

 <b>May 2023</b>: Our paper [On the Copying Problem of Unsupervised NMT: A Training Schedule with a Language Discriminator Loss](https://aclanthology.org/2023.iwslt-1.48.pdf) was accepted to IWSLT 2023 and our paper [Improving Isochronous Machine Translation with Target Factors and Auxiliary Counters](https://www.isca-speech.org/archive/pdfs/interspeech_2023/pal23_interspeech.pdf) (from my Amazon internship) was accepted to Interspeech 2023!

  <b>May 2023</b>: [Jesse](https://jessedodge.github.io/) gave a talk at the [LTI CMU Colloquium](https://lti.cs.cmu.edu/lti-colloquium), discussing our recent papers on efficient domain adaptation of pretrained language models ([1](https://aclanthology.org/2022.naacl-main.96.pdf), [2](https://aclanthology.org/2023.findings-eacl.153.pdf)); you can check it out [here](https://youtu.be/ZFqm7NnRAe0).

  <b>April 2023</b>: Very happy to start a research internship in Google Berlin, as part of Google DeepMind!

 <b>March 2023</b>: Our paper [Language-Family Adapters for Low-Resource Multilingual NMT](https://aclanthology.org/2023.loresmt-1.5.pdf) was accepted to LoResMT, EACL 2023!

 <b>February 2023</b>: Check out or work on [Isochronous Automatic Dubbing](https://arxiv.org/pdf/2302.12979.pdf) (from my internship at Amazon last fall)!

<b>February 2023</b>: I am co-organizing a shared task on [dubbing](https://iwslt.org/2023/dubbing) in [IWSLT 2023](https://iwslt.org/2023/) (co-located with ACL next summer) along with former teammates from Amazon.

<b>January 2023</b>: Our paper [AdapterSoup: Weight Averaging to Improve Generalization of Pretrained Language Models](https://aclanthology.org/2023.loresmt-1.5.pdf) from my internship at Allen AI was accepted to EACL 2023 (findings)!


<!-- <b>October 2022</b>: Our work [m<sup>4</sup> Adapter: Multilingual Multi-Domain Adaptation for Machine Translation with a Meta-Adapter](https://aclanthology.org/2022.findings-emnlp.315.pdf) was accepted to EMNLP 2022 (findings)!

<b>October 2022</b>: Check out our work on [Language-Family Adapters for Low-Resource Multilingual NMT](https://arxiv.org/pdf/2209.15236.pdf)! 

<b>September 2022</b>: Invited talk at [Lilt AI](https://lilt.com/) on our recent NAACL paper.
([slides](https://alexandra-chron.github.io/files/hierdomadapt.pdf))

<b>August 2022</b>: Happy to share that I started an internship at Amazon AI in Santa Clara, California! I will spend this fall working on speech translation with the Doubtfire team. -->

<!-- <b>June 2022</b>: Invited talk at the [DG CNECT workshop](https://lr-coordination.eu/workshop4) on large language models ([slides](https://alexandra-chron.github.io/files/talk_dgcnect.pdf)). -->

<!-- <b>May 2022</b>: Excited to start another internship at Allen AI, working with Jesse Dodge and Matt Peters! -->

<!-- <b>April 2022</b>: Our paper [Efficient Hierarchical Domain Adaptation for Pretrained Language Models](https://aclanthology.org/2022.naacl-main.96.pdf) from my internship in Allen AI was accepted to NAACL 2022 (main track); I wrote a [blog post](https://blog.allenai.org/efficient-hierarchical-domain-adaptation-using-pretrained-language-models-fdd04c001230) about it, give it a read!  -->

<!-- <b>July 2021</b>: Started an internship at Allen AI, working with [Jesse Dodge](https://jessedodge.github.io/) and [Matt Peters](https://scholar.google.com/citations?user=K5nCPZwAAAAJ&hl=en)! -->

<!-- <b>March 2021</b>: 1 paper accepted at NAACL 2021: [Improving the Lexical Ability of Pretrained Language Models for Unsupervised NMT](https://www.aclweb.org/anthology/2021.naacl-main.16.pdf) (main conf.)

<b>September 2020</b>: 2 papers accepted at EMNLP 2020: [Reusing a Pretrained Language Model on Languages with Limited Corpora for Unsupervised NMT](https://aclanthology.org/2020.emnlp-main.214.pdf) (main conf.) and [Domain Adversarial Fine-Tuning as an Effective Regularizer](https://aclanthology.org/2020.findings-emnlp.278.pdf) (findings)

<b>July 2020</b>: Our system ranked first in the WMT 2020 Unsupervised Translation Shared Task (translation system between Upper Sorbian and German). <br> -->

 <h2>Selected Publications</h2>

<!-- [<a href="https://alexandra-chron.github.io/publications/">AdapterSoup: Weight Averaging to Improve Generalization of Pretrained Language Models</a>] -->

<ul class="sparse-list">
            <li>
          <b><a  href="https://arxiv.org/abs/2311.09344.pdf">Language and Task Arithmetic with Parameter-Efficient Layers for Zero-Shot Summarization</a></b> <br/> 
          <b>Alexandra Chronopoulou</b>, <a href="https://pfeiffer.ai/">Jonas Pfeiffer</a>, <a href="https://scholar.google.com/citations?user=LdNDZRcAAAAJ&hl=en">Joshua Maynez</a>, <a href="https://cindyxinyiwang.github.io/">Xinyi Wang</a>, <a href="https://www.ruder.io/">Sebastian Ruder</a>, <a href="https://sites.google.com/site/priyankaagr17">Priyanka Agrawal</a>  <br/>
          arXiv 2023<br/>
        </li>
          <li>
          <b><a  href="https://aclanthology.org/2023.findings-eacl.153.pdf">AdapterSoup: Weight Averaging to Improve Generalization of Pretrained Language Models</a></b> [<a href="https://alexandra-chron.github.io/files/adaptersoup.pdf" class="link-in-list">slides</a>] <br/> 
          <b>Alexandra Chronopoulou</b>, <a href="https://scholar.google.com/citations?user=K5nCPZwAAAAJ&hl=en">Matthew E. Peters</a>, <a href="https://www.cis.uni-muenchen.de/~fraser/">Alexander Fraser</a>, <a href="https://jessedodge.github.io/">Jesse Dodge</a> <br/>
          EACL 2023 (Findings)<br/>
        </li>
         <li>
          <b><a href="https://aclanthology.org/2023.loresmt-1.5.pdf">Language-Family Adapters for Low-Resource Multilingual Neural Machine Translation</a></b> [<a href="https://alexandra-chron.github.io/files/langfam_adapters.pdf" class="link-in-list">slides</a>] <br/>
          <b>Alexandra Chronopoulou</b>, <a href="https://www.cis.lmu.de/~dario/">Dario Stojanovski</a>, <a href="https://www.cis.uni-muenchen.de/~fraser/">Alexander Fraser</a> <br/>
          EACL LoResMT workshop 2023 <br/>
          </li>
            <li>
           <b><a  href="https://aclanthology.org/2022.naacl-main.96.pdf">Efficient Hierarchical Domain Adaptation for Pretrained Language Models</a></b>          
          [<a href="https://github.com/alexandra-chron/hierarchical-domain-adaptation" class="link-in-list">code</a>]
          [<a href="https://blog.allenai.org/efficient-hierarchical-domain-adaptation-using-pretrained-language-models-fdd04c001230">blog</a>]
          [<a href="https://alexandra-chron.github.io/files/eff_hier_dom_adapt.pdf" class="link-in-list">slides</a>] <br/>
          <b>Alexandra Chronopoulou</b>, <a href="https://scholar.google.com/citations?user=K5nCPZwAAAAJ&hl=en">Matthew E. Peters</a>, <a href="https://jessedodge.github.io/">Jesse Dodge</a> <br/>
          NAACL 2022<br/>
          <!-- [<a href="https://aclanthology.org/2022.naacl-main.96.pdf">paper</a>] -->
        </li>
          <li>
          <b><a href="https://aclanthology.org/2021.naacl-main.16.pdf">Improving the Lexical Ability of Pretrained Language Models for Unsupervised Neural Machine Translation</a></b>
          [<a href="https://github.com/alexandra-chron/lexical_xlm_relm" class="link-in-list">code</a>] <br/>
          <b>Alexandra Chronopoulou</b>, <a href="https://www.cis.lmu.de/~dario/">Dario Stojanovski</a>, <a href="https://www.cis.uni-muenchen.de/~fraser/">Alexander Fraser</a> <br/>
          NAACL 2021<br/>
          <!-- [<a href="https://www.aclweb.org/anthology/2021.naacl-main.16.pdf">paper</a>] -->
        </li>
        <li>
          <b><a href="https://aclanthology.org/2020.emnlp-main.214.pdf">Reusing a Pretrained Language Model on Languages with Limited Corpora for Unsupervised NMT</a></b>         [<a href="https://github.com/alexandra-chron/relm_unmt">code</a>]
          [<a href="https://alexandra-chron.github.io/files/relm.pdf" class="link-in-list">slides</a>]<br/>
          <b>Alexandra Chronopoulou</b>, <a href="https://www.cis.lmu.de/~dario/">Dario Stojanovski</a>, <a href="https://www.cis.uni-muenchen.de/~fraser/">Alexander Fraser</a> <br/>
          EMNLP 2020 <br/>
          <!-- [<a href="https://www.aclweb.org/anthology/2020.emnlp-main.214.pdf" class="link-in-list">paper</a>] -->
        </li>
        <li>
          <b><a href="https://www.aclweb.org/anthology/N19-1213.pdf">An Embarrassingly Simple Approach for Transfer Learning from Pretrained Language Models</a></b>  [<a href="https://github.com/alexandra-chron/siatl" class="link-in-list">code</a>] <br/>
          <b>Alexandra Chronopoulou</b>, <a href="https://cbaziotis.github.io/">Christos Baziotis</a>, <a href="https://slp-ntua.github.io/potam/">Alexandros Potamianos</a> <br/>
          NAACL 2019<br/>
          <!-- [<a href="https://www.aclweb.org/anthology/N19-1213.pdf" class="link-in-list">paper</a>] -->
        </li>

</ul>


 <h2>More</h2>

- My undergrad thesis supervisor was [Alexandros Potamianos](https://slp-ntua.github.io/potam/). I spent a good part of 2018 and 2019 in the Speech and Language Processing group of ECE, NTUA. Thesis: [Transfer Learning with Deep Neural Networks for Sentiment Analysis and Semantic Modeling](https://alexandra-chron.github.io/files/thesis_achronopoulou.pdf). During my last undergrad year I was also working as a Machine Learning Engineer at [Behavioral Signal Technologies](https://behavioralsignals.com/). 

- I am from Athens, Greece and I enjoy literature, movies, the sea, summers in Greece, wind-surfing (lately), [Thanasis'](https://www.youtube.com/channel/UC4tLNPEm2HYi2UG7z78MfsQ) concerts, skiing, padel, and travelling. 
