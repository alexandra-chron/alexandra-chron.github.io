---
permalink: /
title: "Alexandra Chronopoulou"
excerpt: "About me"
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---


I am a second-year PhD student in the [Center for Information and Language Processing](https://www.cis.uni-muenchen.de/) at [LMU Munich](https://www.en.uni-muenchen.de/index.html) under the supervision of [Alex Fraser](https://www.cis.uni-muenchen.de/~fraser/). I am broadly interested in transfer learning, self-supervised learning, domain adaptation and low-resource machine translation, in both supervised and unsupervised scenarios. My PhD thesis is currently focused on improving cross-lingual pretraining for low-resource languages. In this line of work, I proposed a curriculum for pretraining masked language models to improve unsupervised translation for high-resource/low-resource language pairs. I have also worked on enhancing cross-lingual pretraining by including lexical information from non-contextualized embeddings. Lately, I have been exploring methods of improving pretraining via denoising auto-encoding leveraging typology information, with the final goal of achieving better translation quality. 

 Before that, I completed my diploma (combined BEng and MEng) at the [National Technical University of Athens](https://www.ntua.gr/en/) in Athens, Greece, department of [Electrical and Computer Engineering](https://www.ece.ntua.gr/en). In my thesis, I explored ways of improving transfer learning with language Modeling for several classification tasks, most notably on emotion recognition, under the supervision of [Alex Potamianos](https://scholar.google.com/citations?user=pBQViyUAAAAJ&hl=en). 

 <h2>News</h2>

<b>March 2021</b>: Our paper "Improving the Lexical Ability of Pretrained Language Models for Unsupervised Neural Machine Translation" has been accepted to appear in NAACL 2021!
