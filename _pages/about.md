---
permalink: /
title: "Hi, I'm Alexandra!"
excerpt: "Alexandra Chronopoulou"
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---


I am a third-year PhD student in the [Center for Information and Language Processing](https://www.cis.uni-muenchen.de/) at [LMU Munich](https://www.en.uni-muenchen.de/index.html) advised by [Alex Fraser](https://www.cis.uni-muenchen.de/~fraser/).

I am interested in self-supervised training for supervised (and unsupervised) machine translation and domain adaptation, mainly using monolingual or cross-lingual language models. My work revolves around using prior knowledge from unsupervised pretraining to improve performance in low-resource scenarios. 

Before coming to Germany, I received my Diploma on [Electrical and Computer Engineering](https://www.ece.ntua.gr/en) from the [National Technical University of Athens](https://www.ntua.gr/en/) (NTUA). My thesis was advised by [Alex Potamianos](https://scholar.google.com/citations?user=pBQViyUAAAAJ&hl=en). During my final undergrad year, I was working in [Behavioral Signals](https://behavioralsignals.com/) as a Machine Learning engineer. 

Recently, I interned with the AllenNLP team of [Allen Institute for AI](https://allenai.org/), where I was lucky to be advised by [Jesse Dodge](https://jessedodge.github.io/) and [Matt Peters](https://scholar.google.com/citations?user=K5nCPZwAAAAJ&hl=en). 


 <h2>News</h2>
 
<b>December 2021</b>: Our pre-print on [efficient domain adaptation of language models](https://arxiv.org/pdf/2112.08786.pdf) is out. This project is the result of my internship in [Allen AI](https://allenai.org/). 

<b>July 2021</b>: Excited to share that I will be starting a research internship in Allen AI in July, working with [Jesse Dodge](http://www.cs.cmu.edu/~jessed/) and [Matt Peters](https://scholar.google.com/citations?user=K5nCPZwAAAAJ&hl=en)!

<b>March 2021</b>: Our paper "Improving the Lexical Ability of Pretrained Language Models for Unsupervised Neural Machine Translation" has been accepted to appear in NAACL 2021!

<b>September 2020</b>: Our paper "Reusing a Pretrained Language Model on Languages with Limited Corpora for Unsupervised NMT" has been accepted to appear in EMNLP 2020! Also, our work "Domain Adversarial Fine-Tuning as an Effective Regularizer" was accepted in the findings of EMNLP.

<b>July 2020</b>: Our system ranked first in the WMT 2020 Unsupervised Translation Shared Task. The task was to create a purely unsupervised machine translation model that translates between Upper Sorbian and German.

 <h2>Selected Publications</h2>

<ul class="sparse-list">
            <li>
          <b>Efficient Hierarchical Domain Adaptation for Pretrained Language Models</b> <br/>
          <u>Alexandra Chronopoulou</u>, Matthew E. Peters and Jesse Dodge. <br/>
          arXiv preprint.<br/>
          [<a href="https://arxiv.org/pdf/2112.08786.pdf">paper</a>]
        </li>
          <li>
          <b>Improving the Lexical Ability of Pretrained Language Models for Unsupervised Neural Machine Translation</b> <br/>
          <u>Alexandra Chronopoulou</u>, Dario Stojanovski and Alexander Fraser. <br/>
          NAACL 2021.<br/>
          [<a href="https://www.aclweb.org/anthology/2021.naacl-main.16.pdf">paper</a>]
          [<a href="https://github.com/alexandra-chron/lexical_xlm_relm" class="link-in-list">code</a>]
        </li>
        <li>
          <b>Reusing a Pretrained Language Model on Languages with Limited Corpora for Unsupervised NMT</b> <br/>
          <u>Alexandra Chronopoulou</u>, Dario Stojanovski and Alexander Fraser. <br/>
          EMNLP 2020.<br/>
          [<a href="https://www.aclweb.org/anthology/2020.emnlp-main.214.pdf" class="link-in-list">paper</a>]
          [<a href="https://github.com/alexandra-chron/relm_unmt" class="link-in-list">code</a>]
          [<a href="https://drive.google.com/file/d/1HJ_5g_TifOSXUpUeHbDg4c3tyhZew_GD/view?usp=sharing" class="link-in-list">slides</a>]
        </li>
        <li>
          <b>Domain Adversarial Fine-Tuning as an Effective Regularizer</b> <br/>
          Giorgos Vernikos, Katerina Margatina, <u>Alexandra Chronopoulou</u> and Ion Androutsopoulos. <br/>
          Findings of ACL: EMNLP 2020.<br/>
          [<a href="https://www.aclweb.org/anthology/2020.findings-emnlp.278.pdf" class="link-in-list">paper</a>]
          [<a href="https://github.com/GeorgeVern/AFTERV1.0" class="link-in-list">code</a>]
        </li>
        <li>
          <b>An Embarrassingly Simple Approach for Transfer Learning from Pretrained Language Models</b> <br/>
          <u>Alexandra Chronopoulou</u>, Christos Baziotis and Alexandros Potamianos. <br/>
          NAACL 2019.<br/>
          [<a href="https://www.aclweb.org/anthology/N19-1213.pdf" class="link-in-list">paper</a>]
          [<a href="https://github.com/alexandra-chron/siatl" class="link-in-list">code</a>]
        </li>

</ul>

