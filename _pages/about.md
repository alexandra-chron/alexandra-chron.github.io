---
permalink: /
title: "Hi, I'm Alexandra!"
excerpt: "Alexandra Chronopoulou"
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---


I am a final year PhD student in the [Center for Information and Language Processing](https://www.cis.uni-muenchen.de/) at [LMU Munich](https://www.en.uni-muenchen.de/index.html) advised by [Alex Fraser](https://www.cis.uni-muenchen.de/~fraser/). My research is in natural language processing and machine learning.
I am interested in self-supervised training for supervised (and unsupervised) machine translation and domain adaptation, mainly using monolingual or cross-lingual language models. My most recent research focuses on parameter-efficient methods for transfer learning.

Currently, I am an Applied Scientist Intern at Amazon Web Services ([AWS](https://aws.amazon.com/machine-learning/language/)) in Santa Clara, California, working with with the AI human language tecnhology group.
In summer 2021 and spring 2022, I was a Research Intern at [Allen Institute for AI](https://allenai.org/allennlp), working with the AllenNLP team. 

Before starting my graduate studies, I obtained a diploma on [Electrical and Computer Engineering](https://www.ece.ntua.gr/en) from the Nat. Tech. University of Athens (NTUA). My thesis advisor was [Alex Potamianos](https://scholar.google.com/citations?user=pBQViyUAAAAJ&hl=en). 



 <h2>News</h2>

<b>October 2022</b>: 2 new papers are out: m^4 Adapter: Multilingual Multi-Domain Adaptation for Machine Translation with a Meta-Adapter (EMNLP 2022 findings) 
and [Language-Family Adapters for Multilingual NMT](https://arxiv.org/pdf/2209.15236.pdf) (arXiv pre-print)

<b>September 2022</b>: Invited talk at [Lilt AI](https://lilt.com/) on our recent NAACL paper ([slides](/files/hierdomadapt.pdf).

<b>August 2022</b>: Happy to share that I started an internship at Amazon AI in Santa Clara, California! I will be working on speech translation with [Prashant Mathur](http://mtresearcher.github.io/) and the rest of the Doubtfire team.

<b>June 2022</b>: Invited talk at the [DG CNECT workshop](https://lr-coordination.eu/workshop4) ([slides](/files/talk_dgcnect.pdf)).

<b>May 2022</b>: Excited to start another internship at Allen AI (working with the same team)!

<b>April 2022</b>: 1 paper accepted at NAACL 2022: [Efficient Hierarchical Domain Adaptation for Pretrained Language Models](https://aclanthology.org/2022.naacl-main.96.pdf) (main conf.)

<b>July 2021</b>: Started an internship at Allen AI, working with [Jesse Dodge](https://jessedodge.github.io/) and [Matt Peters](https://scholar.google.com/citations?user=K5nCPZwAAAAJ&hl=en)!

<b>March 2021</b>: 1 paper accepted at NAACL 2021: [Improving the Lexical Ability of Pretrained Language Models for Unsupervised NMT](https://www.aclweb.org/anthology/2021.naacl-main.16.pdf) (main conf.)

<b>September 2020</b>: 2 papers accepted at EMNLP 2020: [Reusing a Pretrained Language Model on Languages with Limited Corpora for Unsupervised NMT](https://aclanthology.org/2020.emnlp-main.214.pdf) (main conf.) and [Domain Adversarial Fine-Tuning as an Effective Regularizer](https://aclanthology.org/2020.findings-emnlp.278.pdf) (findings)

<b>July 2020</b>: Our system ranked first in the WMT 2020 Unsupervised Translation Shared Task (translation system between Upper Sorbian and German). <br>

 <h2>Selected Publications</h2>

<ul class="sparse-list">
            <li>
          <b>Efficient Hierarchical Domain Adaptation for Pretrained Language Models</b> <br/>
          <u>Alexandra Chronopoulou</u>, Matthew E. Peters and Jesse Dodge. <br/>
          NAACL 2022.<br/>
          [<a href="https://aclanthology.org/2022.naacl-main.96.pdf">paper</a>] [<a href="https://github.com/alexandra-chron/hierarchical-domain-adaptation" class="link-in-list">code</a>] [<a href="https://blog.allenai.org/efficient-hierarchical-domain-adaptation-using-pretrained-language-models-fdd04c001230">blog</a>]
        </li>
          <li>
          <b>Improving the Lexical Ability of Pretrained Language Models for Unsupervised Neural Machine Translation</b> <br/>
          <u>Alexandra Chronopoulou</u>, Dario Stojanovski and Alexander Fraser. <br/>
          NAACL 2021.<br/>
          [<a href="https://www.aclweb.org/anthology/2021.naacl-main.16.pdf">paper</a>]
          [<a href="https://github.com/alexandra-chron/lexical_xlm_relm" class="link-in-list">code</a>]
        </li>
        <li>
          <b>Reusing a Pretrained Language Model on Languages with Limited Corpora for Unsupervised NMT</b> <br/>
          <u>Alexandra Chronopoulou</u>, Dario Stojanovski and Alexander Fraser. <br/>
          EMNLP 2020.<br/>
          [<a href="https://www.aclweb.org/anthology/2020.emnlp-main.214.pdf" class="link-in-list">paper</a>]
          [<a href="https://github.com/alexandra-chron/relm_unmt" class="link-in-list">code</a>]
          [<a href="https://drive.google.com/file/d/1HJ_5g_TifOSXUpUeHbDg4c3tyhZew_GD/view?usp=sharing" class="link-in-list">slides</a>]
        </li>
        <li>
          <b>An Embarrassingly Simple Approach for Transfer Learning from Pretrained Language Models</b> <br/>
          <u>Alexandra Chronopoulou</u>, Christos Baziotis and Alexandros Potamianos. <br/>
          NAACL 2019.<br/>
          [<a href="https://www.aclweb.org/anthology/N19-1213.pdf" class="link-in-list">paper</a>]
          [<a href="https://github.com/alexandra-chron/siatl" class="link-in-list">code</a>]
        </li>

</ul>

