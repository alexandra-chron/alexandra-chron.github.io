---
layout: archive
title: "Publications"
permalink: /publications/
author_profile: true
---

  You can also find my articles on my [Semantic Scholar](https://www.semanticscholar.org/author/Alexandra-Chronopoulou/3379701) or [Google Scholar](https://scholar.google.com/citations?hl=en&user=XiwRCRIAAAAJ&view_op=list_works&sortby=pubdate) profile.

Publications and arxiv preprints: 

<ul class="sparse-list">
          <li>
          <b>m<sup>4</sup> Adapter: Multilingual Multi-Domain Adaptation for Machine Translation with a Meta-Adapter</b> <br/>
          Wen Lai, <u>Alexandra Chronopoulou</u> and Alexander Fraser. <br/>
          Findings of ACL: EMNLP 2022.<br/>[<a href="https://arxiv.org/abs/2210.11912">paper</a>]
          </li>
            <li>
          <b>Language-Family Adapters for Multilingual Neural Machine Translation</b> <br/>
          <u>Alexandra Chronopoulou</u>, Dario Stojanovski and Alexander Fraser. <br/>
          arxiv preprint.<br/>[<a href="https://arxiv.org/pdf/2209.15236.pdf">paper</a>]
          </li>
            <li>
          <b>Efficient Hierarchical Domain Adaptation for Pretrained Language Models</b> <br/>
          <u>Alexandra Chronopoulou</u>, Matthew E. Peters and Jesse Dodge. <br/>
          NAACL 2022.<br/>
          [<a href="https://aclanthology.org/2022.naacl-main.96.pdf">paper</a>]
          [<a href="https://github.com/alexandra-chron/hierarchical-domain-adaptation" class="link-in-list">code</a>]
          [<a href="https://blog.allenai.org/efficient-hierarchical-domain-adaptation-using-pretrained-language-models-fdd04c001230">blog</a>]
          [<a href="/files/eff_hier_dom_adapt.pdf" class="link-in-list">slides</a>] 
        </li>
          <li>
          <b>Improving the Lexical Ability of Pretrained Language Models for Unsupervised Neural Machine Translation</b> <br/>
          <u>Alexandra Chronopoulou</u>, Dario Stojanovski and Alexander Fraser. <br/>
          NAACL 2021.<br/>
          [<a href="https://www.aclweb.org/anthology/2021.naacl-main.16.pdf">paper</a>]
          [<a href="https://github.com/alexandra-chron/lexical_xlm_relm" class="link-in-list">code</a>]
        </li>
        <li>
          <b>Reusing a Pretrained Language Model on Languages with Limited Corpora for Unsupervised NMT</b> <br/>
          <u>Alexandra Chronopoulou</u>, Dario Stojanovski and Alexander Fraser. <br/>
          EMNLP 2020.<br/>
          [<a href="https://www.aclweb.org/anthology/2020.emnlp-main.214.pdf" class="link-in-list">paper</a>]
          [<a href="https://github.com/alexandra-chron/relm_unmt" class="link-in-list">code</a>]
          [<a href="/files/relm.pdf" class="link-in-list">slides</a>]
        </li>
        <li>
          <b>Domain Adversarial Fine-Tuning as an Effective Regularizer</b> <br/>
          Giorgos Vernikos, Katerina Margatina, <u>Alexandra Chronopoulou</u> and Ion Androutsopoulos. <br/>
          Findings of ACL: EMNLP 2020.<br/>
          [<a href="https://www.aclweb.org/anthology/2020.findings-emnlp.278.pdf" class="link-in-list">paper</a>]
          [<a href="https://github.com/GeorgeVern/AFTERV1.0" class="link-in-list">code</a>]
        </li>
        <li>
          <b>The LMU Munich System for the WMT 2020 Unsupervised Machine Translation Shared Task</b> <br/>
          <u>Alexandra Chronopoulou</u>, Dario Stojanovski, Viktor Hangya and Alexander Fraser. <br/>
          WMT 2020.<br/>
          [<a href="https://www.aclweb.org/anthology/2020.wmt-1.128.pdf" class="link-in-list">paper</a>]
          [<a href="https://github.com/alexandra-chron/umt-lmu-wmt2020" class="link-in-list">code</a>]
          [<a href="https://drive.google.com/file/d/1gZvhZd5TW3z7VJubts13Y35L8H1mbBT1/view?usp=sharing" class="link-in-list">slides</a>]
         [<a href="https://drive.google.com/file/d/1bTjshwr8amPLyxlPzVDEcB6DtA4vjVUX/view?usp=sharing" class="link-in-list">poster</a>]
        </li>
        <li>
          <b>Transfer Learning with Deep Neural Networks for Sentiment Analysis and Semantic Modeling</b> <br/>
          <u>Alexandra Chronopoulou</u>. <br/>
          National Technical University of Athens.<br/>
          [<a href="/files/thesis_achronopoulou.pdf">thesis (in English)</a>] [<a href="https://dspace.lib.ntua.gr/xmlui/bitstream/handle/123456789/49039/diplomatiki_achronopoulou.pdf?sequence=1" class="link-in-list">thesis (in Greek)</a>]
        </li>
        <li>
          <b>An Embarrassingly Simple Approach for Transfer Learning from Pretrained Language Models</b> <br/>
          <u>Alexandra Chronopoulou</u>, Christos Baziotis and Alexandros Potamianos. <br/>
          NAACL 2019.<br/>
          [<a href="https://www.aclweb.org/anthology/N19-1213.pdf" class="link-in-list">paper</a>]
          [<a href="https://github.com/alexandra-chron/siatl" class="link-in-list">code</a>]
        </li>
        <li>
          <b>NTUA-SLP at IEST 2018: Ensemble of neural transfer methods for implicit emotion classification</b> <br/>
          <u>Alexandra Chronopoulou</u>, Katerina Margatina, Christos Baziotis and Alexandros Potamianos. <br/>
          WASSA 2018.<br />
          [<a href="https://www.aclweb.org/anthology/W18-6209.pdf" class="link-in-list">paper</a>]
          [<a href="https://github.com/alexandra-chron/ntua-slp-wassa-iest2018">code</a>]
        </li>
        <li>
          <b>NTUA-SLP at SemEval-2018 Task 1: Predicting Affective Content in Tweets with Deep Attentive RNNs and Transfer Learning</b> <br/>
          Christos Baziotis, Nikos Athanasiou, <u>Alexandra Chronopoulou</u>, Athanasia Kolovou, Georgios Paraskevopoulos, Nikolaos Ellinas, Shrikanth Narayanan and  Alexandros Potamianos. <br/>
          SemEval 2018. <br />
          [<a href="https://www.aclweb.org/anthology/S18-1037.pdf" class="link-in-list">paper</a>]
          [<a href="https://github.com/cbaziotis/ntua-slp-semeval2018" class="link-in-list">code</a>]
        </li>
</ul>
